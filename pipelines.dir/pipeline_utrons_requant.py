##############################################################################
#
#   MRC FGU CGAT
#
#   $Id$
#
#   Copyright (C) 2009 Andreas Heger
#
#   This program is free software; you can redistribute it and/or
#   modify it under the terms of the GNU General Public License
#   as published by the Free Software Foundation; either version 2
#   of the License, or (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program; if not, write to the Free Software
#   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
###############################################################################
"""
===========================
Pipeline template
===========================

:Author: Ian Sudbery
:Release: $Id$
:Date: |today|
:Tags: Python
:Updated for usage with python3 by: Cristina Alexandru
:Updated 24/11/2020 by Jack Riley to remove references to deprecated cgatpipelines.report module. Commented out lines: 170, 776-806
:Updated 29/06/2022 by Ian Sudbery: forked to pull out functions for requantify samples against a pre-created gtf
Overview
========

This pipeline quantifies the usage of introns in the 3' UTR (3UI/Utron) from previously
assembled and annotated GTF and utron_bed files. 

The pipeline performs the following:

    * Makes a Salmon index and quantifies transcripts with Salmon.
    * Merges all quantification files and uploads the outputs onto database.
    * Identify the percent-spliced out for every 3UI
    * Count number of reads in each exon and crossing each junction
    * (Optionally) run rMATs to compare between two different sets of samples

Background
============

Salmon is run in 
Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.ini` file.
CGATReport report requires a :file:`conf.py` and optionally a
:file:`cgatreport.ini` file (see :ref:`PipelineReporting`).

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_utrons.py config

Input files
-----------

1. A previously assembled GTF file, e.g. one made with pipeline_utrons
2. A tx2gene file generated from the above GTF file
3. Either bam with reads of RNAseq samples, or remote files with links to 
   the download location for such files. 

Requirements
------------

The pipeline requires the results from
:doc:`pipeline_annotations`. Set the configuration variable
:py:data:`annotations_database` and :py:data:`annotations_dir`.

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

.. Add any additional external requirements such as 3rd party software
   or R modules below:

Requirements:

The pipeline requires the following software to be in the path:

+--------------+----------+------------------------------------+
|*Program*     |*Version* |*Purpose*                           |
+--------------+----------+------------------------------------+
|gff2fasta     |          |bulding transcriptome in .fa format |
+--------------+----------+------------------------------------+
|salmon_       |>=1.0.0   |alignment free quantitaition        |
+--------------+----------+------------------------------------+
|featureCounts |>=2.0.1   |Counting reads and junctions        |
+--------------+----------+------------------------------------+
|samtools      |>=1.12    | bam file manipulation              |
+--------------+----------+------------------------------------+
|cgat-apps     |>=0.6.4   |Various                             |
+--------------+----------+------------------------------------+
|rMATs         |>=4       |(optional) DJU analysis             |
+--------------+----------+------------------------------------+

for full requiredments see accumpanying conda environment files. 

Pipeline output
===============

1. A salmon index
2. Salmon quantification files in .sf format are generted in quantification.dir.
3. A file with the number of reads in each exon
4. A file with the number of reads cross each junction
5. A file with the percent spliced out for every utron/3UI
6. A file with the TPM and expression fraction of every transcript in every
   sample. 
   

Glossary
========

.. glossary::

salmon
      salmon_ - alignment-free quantification

.. _salmon: https://combine-lab.github.io/salmon/

###########################################################################


Code
====

"""
from genericpath import isdir
from ruffus import *
from ruffus.combinatorics import product
import sys
import os
import shutil
import glob
import re
from cgatcore import experiment as E
import cgat.Sra as Sra
from cgatcore import pipeline as P
from cgatcore import iotools
import tempfile

# load options from the config file
PARAMS = P.get_parameters(
    ["%s/pipeline.yml" % os.path.splitext(__file__)[0],
     "../pipeline.yml",
     "pipeline.yml"])

# add configuration values from associated pipelines
#
# 1. pipeline_annotations: any parameters will be added with the
#    prefix "annotations_". The interface will be updated with
#    "annotations_dir" to point to the absolute path names.

PARAMS["project_src"]=os.path.dirname(__file__)


# -----------------------------------------------
# Utility functions
def connect():
    '''utility function to connect to database.

    Use this method to connect to the pipeline database.
    Additional databases can be attached here as well.
    Returns an sqlite3 database handle.
    '''

    dbh = sqlite3.connect(PARAMS["database_name"])
    statement = '''ATTACH DATABASE '%s' as annotations''' % (
        PARAMS["annotations_database"])
    cc = dbh.cursor()
    cc.execute(statement)
    cc.close()

    return dbh


def toParquet(infile=None,
              infiles=None,
              outfile=None,
              tablename=None,
              key_columns=None,
              no_header=False,
              col_names=None,
              partition=False,
              partition_on=None,
              regex_filename=None,
              sep=None,
              unzip=False,
              dtypes=None,
              **kwargs):
    '''Convert an infile, or a list of infiles to parquet format. The files
    will be put in a directory determined by the parquet path in the pipeline
    configuration, plus the tablename provided. If no tablename is provided,
    the the outfile will be used to determine one. The log will be put in the
    outfile. 
    
    The script uses Dask to keep memory usage low, but this only works if
    input is unzipped otherwise each input is loaded into memory in one go.
    If indevidual files are larger than memory, use unzip=True: the input 
    files will be unzipped to a temporary location first.
    '''
    
    script_path = os.path.join(os.path.dirname(__file__),
                               "pipeline_utrons",
                               "csvs2parquet.py")
    
    if tablename is None:
        try:
            tablename = P.snip(os.path.basename(outfile), ".load")
        except ValueError:
            tablename = os.path.basename(outfile)
    
    
    if not os.path.exists(PARAMS["database_parquet_root"]):
        os.mkdir(PARAMS["database_parquet_root"])
        
        
    outpath = os.path.join(PARAMS["database_parquet_root"],
                           tablename)
         
    if infile is not None and infiles is not None:
        raise ValueError("Specify only one of infile or infiles")
    
    if infile is not None:
        infiles = [infile]
    else:
        if key_columns is None:
            key_columns = "track"
              
    options = "--output-prefix=%s" % outpath
    
    if key_columns is not None:
        options += " --key-columns=%s" % key_columns
    
    if no_header:
        options += " --no-header"
        
    if col_names:
        options += " --col-names=%s" % col_names
        
    if partition_on is not None:
        options += " -P --partition_on=%s" % partition_on
    elif partition is True:
        options += " -P"
        
    if regex_filename is not None:
        options += " --regex-filename='%s'" % regex_filename
        
    if dtypes is not None:
        options += " --dtypes='%s'" % dtypes
        
    if sep is not None:
        options += " --sep='%s'" % sep
    
    if "job_memory" in kwargs:
        job_memory = kwargs["job_memory"]
    
    if unzip is True:
        unzipped_names = [os.path.join("$TMPDIR", os.path.basename(P.snip(inf, ".gz"))) 
                          if inf.endswith(".gz") 
                          else os.path.join("$TMPDIR", os.path.basename(inf))
                          for inf in infiles]
                
        statement = " && ".join("zcat %s | grep -v '^#' > %s" % (infi, out) for infi, out in
                                 zip(infiles, unzipped_names)) + " && "
        
        infiles = unzipped_names
        
    else:
        statement = ""
        
    infiles = " ".join(infiles)
    
    statement += '''
        python %(script_path)s
               %(options)s
               %(infiles)s -v5 &> %(outfile)s'''
               
    P.run(statement)
    
    
@P.cluster_runnable
def create_index(table, columns):
    ''' utility function to create table indexes. Supply a list of columns
    to create a join index on those columns'''
    
    dbh = connect()
    cc = dbh.cursor()
    
    index_name = table + "_index_" + "_".join(columns)
    columns = ",".join(columns)
    cc.execute("CREATE INDEX %(index_name)s ON %(table)s(%(columns)s)" 
               % locals())
    
STRINGTIE_QUANT_FILES=["i_data.ctab", "e_data.ctab", "t_data.ctab",
                       "i2t.ctab", "e2t.ctab"]


#-----------------------------------------------------------------------------
@follows(mkdir("expression.dir"))
@transform(PARAMS["input_tx2gene"],
           regex(".+"),
           "expression.dir/tx2gene.tsv")
def copy_tx2gene(infile, outfile):
    
    shutil.copy(infile, outfile)
    
# ---------------------------------------------------
@follows(mkdir("salmon_index"), mkdir("salmon_index/DAT"))
@transform(PARAMS["input_gtf"],
           regex(".+/(.+).gtf.gz"),
           r"salmon_index/\1.salmon.index")
def makeSalmonIndex(infile,outfile):
    # Long transcripts cause indexing to use lots of memory?
    job_memory=PARAMS["salmon_index_memory"]
    job_threads=PARAMS["salmon_index_threads"]

    gtf_basename = P.snip(os.path.basename(infile), ".gtf.gz")
    transcript_fasta = "salmon_index/" + gtf_basename + "transcripts.fa"
    fastaref = os.path.join(PARAMS["genome_dir"], 
                            PARAMS["genome"] + ".fa")
    index_options=PARAMS["salmon_indexoptions"]
    tmpfile = P.get_temp_filename()
    
    #statement since salmon >v1.0 Selective Alignment update
    #statement now generates decoy.txt from reference genome, and concats the genome.fa and transcriptome.fa into gentrome.fa
    #gentrome.fa is then passed through salmon alongside the decoys to create decoy-aware transcriptome (DAT)
    statement = '''
    gunzip -c %(infile)s > %(tmpfile)s &&
    gffread %(tmpfile)s -g %(fastaref)s -w %(transcript_fasta)s &&
    grep "^>" <%(fastaref)s | cut -d " " -f 1 > salmon_index/DAT/decoys.txt &&
    sed -i.bak -e 's/>//g' salmon_index/DAT/decoys.txt &&
    cat %(transcript_fasta)s %(fastaref)s > salmon_index/DAT/gentrome.fa.gz &&
    salmon index
      -p %(job_threads)s
      %(index_options)s
      -t salmon_index/DAT/gentrome.fa.gz
      -d salmon_index/DAT/decoys.txt
      -i %(outfile)s &&
    rm %(tmpfile)s
    '''
    P.run(statement)


#---------------------------------------------------------

@follows(mkdir("quantification.dir"), mkdir("sorted_bams"))
@transform(["input_quantify.dir/*.bam",
            "input_quantify.dir/*.remote"],
           formatter(".+/(?P<TRACK>.+).(bam|remote)"),
           add_inputs(makeSalmonIndex),
           "quantification.dir/{TRACK[0]}.agg-agg-agg.filtered/quant.sf")
def quantifyWithSalmon(infiles, outfile):
    '''Quantify existing samples against genesets'''
    job_threads=PARAMS["salmon_threads"]
    job_memory=PARAMS["salmon_memory"]

    # limit to 8 hours - if a job get stuck. Kill it rather than waiting forever
    job_options = PARAMS["cluster_options"] 
    infile, salmonIndex = infiles
    outdir = os.path.dirname(outfile)
    basefile = os.path.basename(infile)
    sample_name = basefile.split(os.extsep, 1)
    salmon_options=PARAMS["salmon_quantoptions"]

    sorted_bam="sorted_bams/" + sample_name[0] + "_sorted.bam"
    possorted_local="$TMPDIR/" + sample_name[0] + "_possorted.bam"
    possorted_bam="sorted_bams" + sample_name[0] + "_possorted.bam"
    fastq1 = os.path.join("$TMPDIR/", sample_name[0]) + ".1.fastq"
    fastq2 = os.path.join("$TMPDIR", sample_name[0]) + ".2.fastq"
    fastq0 = os.path.join("$TMPDIR", sample_name[0]) + ".0.fastq"
    local_index_path = os.path.join("$TMPDIR", "salmon_index")
    
    statement = '''
    samtools collate %(infile)s -o %(sorted_bam)s  $TMPDIR &&
    samtools fastq
         -1 %(fastq1)s
         -2 %(fastq2)s
         -0 %(fastq0)s -s /dev/null -n -F 0x900
         -@ %(job_threads)s
         %(sorted_bam)s &&
    samtools sort %(infile)s -o %(possorted_local)s
                -@ %(job_threads)s 
                -T $TMPDIR &&
    samtools index %(possorted_local)s &&
    mv %(possorted_local)s %(possorted_local)s.bai sorted_bams &&                
    paired_end_reads=$(samtools view -c -f 1 %(sorted_bam)s) &&
    cp -r %(salmonIndex)s %(local_index_path)s &&
    if [ $paired_end_reads = 0 ]; then
        salmon quant -i %(local_index_path)s
            --libType IU
            -r %(fastq0)s
            -o %(outdir)s
            -p %(job_threads)s
            %(salmon_options)s;
    else
        salmon quant -i %(local_index_path)s
            --libType IU
            -1 %(fastq1)s
            -2 %(fastq2)s
            -o %(outdir)s
            -p %(job_threads)s
            %(salmon_options)s; 
    fi &&
    rm %(fastq1)s && rm %(fastq2)s && rm %(fastq0)s && rm -r %(local_index_path)s
    '''

    if infile.endswith(".remote"):
        token = glob.glob("gdc-user-token*")

        tmpfilename = os.path.join("$TMPDIR",
                                   os.path.basename(P.get_temp_filename()))
        if os.path.exists(tmpfilename):
            os.unlink(tmpfilename)    	
        if len(token) > 0:
            token = token[0]
        else:
            token = None
        token = token + " --wait-time=20 -n %(job_threads)s "
        
        s, infile = Sra.process_remote_BAM(
            infile, token, tmpfilename,
            filter_bed=os.path.join(
                PARAMS["annotations_dir"],
                "assembly.dir/contigs.bed"))

        infile = " ".join(infile)
        statement = "&& ".join(
            ["mkdir -p %(tmpfilename)s",
             s,
             statement,
             "rm -r %(tmpfilename)s"])

    P.run(statement)

#----------------------------------------------------
@follows(quantifyWithSalmon)
@merge("quantification.dir/*_agg_agg_agg/logs/salmon_quant.log",
       "mapping_rates.txt")
def merge_mapping_rates(infiles, outfile):
    '''Find the mapping rates in the salmon logs'''

    with open(outfile, "w") as outf:
        for inf in infile:
            for line in open(infile):
                if re.match("Mapping rate", line):
                    outf.write(line)


#----------------------------------------------------
# ***********
# NOTE - last time I used this, the output database wasn't indexed correctly
# ***********
@merge(quantifyWithSalmon, "database_load/salmon_quant.load")
def mergeAllQuants(infiles, outfile):

    P.concatenate_and_load(infiles, outfile,
                         regex_filename="quantification.dir/(.*)_agg-agg-agg.sf",
                         options="-i Name -i Length -i EffectiveLength"
                         " -i TPM -i NumReads -i track"
                         " -i source",
                         job_memory="64G")


#-----------------------------------------------------
@transform(quantifyWithSalmon,
           regex("quantification.dir/(.+?)\.(.+)/quant.sf"),
           inputs(r"sorted_bams/\1_sorted.bam"),
           r"sorted_bams/\1_possorted.bam")
def sortAndIndexBams(infile, outfile):
    '''Index the bam files produced as a side effect of quantify with salmon'''
    
    statement = '''samtools sort %(infile)s -o %(outfile)s &&
                   samtools index %(outfile)s'''
    P.run(statement)
    
    
@follows(mkdir("pso.dir"))
@transform(quantifyWithSalmon,
           regex("quantification.dir/(.+?)\.(.+)/quant.sf"),
           inputs([r"sorted_bams/\1_possorted.bam",
                   os.path.join(PARAMS["input_utron_beds"],
                                "%s.all_utrons.bed.gz" %
                                os.path.basename(PARAMS["input_gtf"]).split(".")[0])]),
           r"pso.dir/\1_all_utrons.pso.tsv.gz")
def calculatePSO(infiles, outfile):
    '''Calculate the percent spliced out for the utron intervals'''
    
    script = os.path.join(PARAMS["project_src"], "pipeline_utrons/get_psi.py")
    bamfile, bedfile = infiles
    
    statement = '''
                    python %(script)s 
                       -I %(bedfile)s
                       -S %(outfile)s
                       -L %(outfile)s.log
                       %(bamfile)s'''
                       
    P.run(statement, job_memory="6G")
   

@merge(calculatePSO, "pso.dir/pso.load")
def load_PSO(infiles, outfile):
    
    toParquet(infiles=infiles,
              outfile=outfile,
              no_header=True,
              col_names="chr,start,end,transcript_id,retained,spliced,incompatible,total,pso",
              regex_filename='.+/(.+)_all_utrons')

    
@follows(mkdir("featureCounts.dir"))
@subdivide(quantifyWithSalmon, 
           regex("quantification.dir/(.+?)\.(.+)/quant.sf"),
           inputs([r"sorted_bams/\1_sorted.bam",
                   PARAMS["input_gtf"]]),
           [r"featureCounts.dir/\1.tsv.gz",
            r"featureCounts.dir/\1.tsv.jcounts"]) 
def countExonsAndJunctions(infiles, outfiles):
    '''Use featureCounts to count both junctions and exons in the 
    assemblyed geneset'''
    
    bamfile, gtffile = infiles
    fastaref = os.path.join(PARAMS["genome_dir"], 
                            PARAMS["genome"] + ".fa")
    job_memory="4G"
    outfile = P.snip(outfiles[0], ".gz")
    statement = '''featureCounts -a %(gtffile)s 
                                 -o %(outfile)s
                                 -f
                                 -J
                                 -O
                                 -p
                                 -G %(fastaref)s
                                 %(bamfile)s &> %(outfile)s.log &&
                                 gzip %(outfile)s
                                 '''
                                 
    P.run(statement)


@collate(countExonsAndJunctions,
         regex("featureCounts.dir/(.+)\.tsv\.(gz|jcounts)"),
         r"featureCounts.dir/featurecounts.\2.load",
         r"\2")
def load_exon_counts(infiles, outfile, track):
    
    if track == "jcounts":
        outname = "featurecounts_jcounts"
        dtypes="PrimaryGene=object,SecondaryGenes=object"
        col_names="PrimaryGene,SecondaryGenes,Site1_chr,Site1_location,Site1_strand,Site2_chr,Site2_location,Site2_strand,count"
        gunzip=False
    else:
        outname = "featurecounts_exons"
        dtypes="gene_id=object,chr=object,start=uint64,end=uint64,start=object,length=uint32,count=uint32"
        col_names="gene_id,chr,start,end,strand,length,count"
        gunzip=False
        
    toParquet(infiles=infiles,
              outfile=outfile,
              tablenamename=outname,
              col_names=col_names,
              unzip=gunzip,
              dtypes=dtypes,
              job_memory="12G")
    
    
@active_if(PARAMS["rmats_run"])
@follows(mkdir("rmats.dir"), mkdir("rmats.dir/prep_files.dir"),
         mkdir("rmats.dir/tmp"))
@transform(quantifyWithSalmon, 
           regex("quantification.dir/(.+?)\.(.+)/quant.sf"),
           inputs([r"sorted_bams/\1_sorted.bam",
                   PARAMS["input_gtf"]]),
           r"rmats.dir/prep_files.dir/\1.rmats",
           r"\1")
def run_rmats_pre(infiles, outfile, track):

    infile, gtffile = infiles
    fixed_event_path = PARAMS["rmats_fixed_event_dir"]
    
    od = os.path.abspath("rmats.dir")
    
    statement = '''rmats.py
                   --task prep
                   --tmp=rmats.dir/%(track)s.dir/
                   --gtf <(zcat %(gtffile)s)
                   --readLength %(rmats_readLength)s
                   --variable-read-length
                    --allow-clipping
                   -t %(rmats_paired)s
                   --od rmats.dir
                   --b1 <(echo %(infile)s)
                   --fixed-event-set=%(fixed_event_path)s
                   &> rmats.dir/%(track)s.prep.log
                   '''                          
                   
    
    P.run(statement, 
          job_condaenv=PARAMS["rmats_env"],
          job_memory = PARAMS["rmats_prep_memory"])

    rmats_counter = ""    
    for f_path in glob.glob("rmats.dir/%(track)s.dir/*.rmats" % locals()):
        shutil.copy(f_path, 
                    P.snip(outfile, ".rmats") + str(rmats_counter) + ".rmats")
        if rmats_counter == "":
            rmats_counter = 1
        else:
            rmats_counter += 1
 

@active_if(PARAMS["rmats_run"])
@collate(run_rmats_pre,
         regex(PARAMS["rmats_collate_pattern"]),
         add_inputs(os.path.join(PARAMS["rmats_fixed_event_dir"],
                                 "fromGTF.RI.txt"),
                    PARAMS["input_gtf"]),
       r"rmats.dir/\1/RI.MATS.JC.txt")
def run_rmats_post(infiles, outfile):
    '''Run rMATs post on the samples if specified in the configuration. Even
    though this is run on a fixed event set, it is a very time consuming
    and memory intensive process. The two different conditions are selected
    using a regex supplied in the configureation'''
    
    fixed_event_path = PARAMS["rmats_fixed_event_dir"]
    gtffile = infiles[0][-1]
    pre_files = [i[0] for i in infiles]
    E.debug(pre_files)
    normal_regex = PARAMS["rmats_normal_regex"]
    test_regex = PARAMS["rmats_test_regex"]
    normals = list()
    test = list()
    
    for infile in pre_files:
        track = os.path.basename(infile)
        track = P.snip(track, ".rmats")
        if re.match(normal_regex, track):
            normals.append("sorted_bams/%s_sorted.bam" % track)
        elif re.match(test_regex, track):
            test.append("sorted_bams/%s_sorted.bam" % track)
        else:
            E.warn("File %s does match either the normal (%s) or "
                    "test (%s) regexes" % 
                    (track, normal_regex, test_regex))
       
    outdir = os.path.dirname(outfile)
    inputdir = os.path.dirname(pre_files[0])     
    normals = ",".join(normals)
    test = ",".join(test)

    if not os.path.isdir(outdir):
        os.mkdir(outdir)

    with iotools.open_file(os.path.join(outdir, "normals.txt"), "w") as norm:
        norm.write(normals + "\n")
        
    with iotools.open_file(os.path.join(outdir, "test.txt"), "w") as test_file:
        test_file.write(test + "\n")
        
    statement = '''rmats.py 
                   --b1 %(outdir)s/test.txt
                   --b2 %(outdir)s/normals.txt
                   --gtf <(zcat %(gtffile)s)
                    -t %(rmats_paired)s
                   --readLength %(rmats_readLength)s
                   --variable-read-length
                   --allow-clipping
                   --nthread %(rmats_post_threads)s
                   --od %(outdir)s
                   --tmp %(inputdir)s
                   --task post
                   --fixed-event-set %(fixed_event_path)s  2>&1 > %(outfile)s.log &&
                   touch %(outfile)s
                   '''
                   
    P.run(statement,
          job_threads=PARAMS["rmats_post_threads"],
          job_memory=PARAMS["rmats_post_memory"],
          job_condaenv=PARAMS["rmats_env"])
  
@active_if(PARAMS["rmats_run"])
@merge(run_rmats_post,
           "rmats.dir/rmate_ri.load")
def load_rmats_ri(infiles, outfile):
    
    toParquet(infiles = infiles,
              outfile=outfile,
              regex_filename="rmats.dir/(.+)\/.+",
              tablename="rMATS_ri")
    
    
@P.cluster_runnable    
def get_rmats_counts_per_junction(infiles, outfile):
    '''Take the comma  junction counts from the rMATs output and produce a 
    tidy dataset with transcript_id, position, sample_id and counts.
    Everything is done with file iterators as its all too big for dataframes'''
    
    from cgat import Bed

    infile, tx2gene, tests, normals, utrons = infiles
    
    track_pattern = re.compile(".+/(.+)_sorted")
    tests = open(tests).readlines()[0].strip().split(",")
    tests = [track_pattern.match(t).groups()[0] for t in tests]
    
    normals = open(normals).readlines()[0].strip().split(",")
    if normals != ['']:    
        normals = [track_pattern.match(t).groups()[0] for t in normals]
    else: 
        normals = list()
           
    def _bed12toBed6():
        for entry in Bed.iterator(iotools.open_file(utrons)):
            for start, length in zip(entry.blockStarts.split(","),
                                 entry.blockSizes.split(",")):
        
                yield ({"contig": entry.contig, 
                        "start": str(entry.start + int(start)),
                        "end": str(entry.start + int(start) + int(length)),
                        "strand": entry.strand,
                        "transcript": entry.name})
                                     
        
    outfile = iotools.open_file(outfile, "w")
    outfile.write("\t".join(["transcript_id",
                            "track",
                            "condition",
                            "contig", 
                            "start",
                            "end",
                            "strand",
                            "spliced_reads",
                            "retained_reads",
                            "PSI"]) + "\n")
    # rmats RI file columns here:
    # https://github.com/Xinglab/rmats-turbo#output
    contig, strand, start, end, inc1_reads, inc2_reads, spl1_reads, spl2_reads, inc_level1, inc_level2 = \
        (3, 4, 8, 9, 12, 14, 13, 15, 20, 21)
         
    rsem_file = iotools.open_file(infile)
    header = rsem_file.readline()
    line = rsem_file.readline()
    
    for utron in _bed12toBed6():   
            
        fields = line.strip().split()
        if line != "" and fields[contig] == utron["contig"] and \
            fields[start] == utron["start"] and \
            fields[end] == utron["end"]:
        
            this_inc1_reads = fields[inc1_reads].split(",")
            this_inc2_reads = fields[inc2_reads].split(",")
            this_spl1_reads = fields[spl1_reads].split(",")
            this_spl2_reads = fields[spl2_reads].split(",")
            this_inc_level1 = fields[inc_level1].split(",")
            this_inc_level2 = fields[inc_level2].split(",")
            line = rsem_file.readline()
            
        else:
            this_inc1_reads = \
                this_spl1_reads = \
                ["0"]*len(tests)
        
            this_inc2_reads = \
                this_spl2_reads = \
                ["0"]*len(normals)
                
            this_inc_level1 = [""]*len(tests)
            this_inc_level2 = [""]*len(normals)
            
        for i in range(len(tests)):
            outline = [utron["transcript"], 
                        tests[i], 
                        "test", 
                        utron["contig"],
                        utron["start"],
                        utron["end"],
                        utron["strand"],
                        this_spl1_reads[i],
                        this_inc1_reads[i],
                        this_inc_level1[i]]
            outfile.write("\t".join(outline) + "\n")
                
        for i in range(len(normals)):
            
            outline =[utron["transcript"], 
                      normals[i], 
                      "normal", 
                      utron["contig"],
                      utron["start"],
                      utron["end"],
                      utron["strand"],
                      this_spl2_reads[i],
                      this_inc2_reads[i],
                      this_inc_level2[i]]
            outfile.write("\t".join(outline) + "\n")
                
    outfile.close()
    
@active_if(PARAMS["rmats_run"])        
@transform(run_rmats_post,
           formatter(),
           add_inputs(copy_tx2gene,
                      "{path[0]}/test.txt",
                      "{path[0]}/normals.txt",
                       os.path.join(PARAMS["input_utron_beds"],
                                    os.path.basename(PARAMS["input_gtf"]).split(".")[0]
                                                     + ".all_utrons.bed.gz")),
           "rmats.dir/{subdir[0][0]}_counts_per_junction.tsv.gz")
def run_get_rmats_counts_per_junction(infiles, outfile):
    
    get_rmats_counts_per_junction(infiles, outfile, submit=True)
 
@active_if(PARAMS["rmats_run"])
@merge(run_get_rmats_counts_per_junction,
           r"rmats.dir/counts_per_junction.load")
def load_rmats_per_junction(infiles, outfile):
    
    toParquet(infiles=infiles,
              regex_filename=".+/(.+)_counts_per_junction",
              outfile=outfile,
              key_columns="design",
              tablename="rmats_junction_counts",
              job_memory="12G")
              
              

@follows(run_rmats_post,
         load_exon_counts,
         load_rmats_ri,
         load_PSO,
         load_rmats_per_junction)
def requant():
    pass            



  
#------------------------------------------------------------------------------
@follows(mkdir("expression.dir"))
@transform(PARAMS["database_false_positives"],
           regex(".+"),
           r"expression.dir/false_positives.txt")
def copy_false_positives(infile, outfile):
    '''copy the file that contains transcripts that get a false positive 3UI
    in simulations to a location when the R script can find it.'''
    
    shutil.copy(infile, outfile)
    

#------------------------------------------------------------------------------    
@merge([quantifyWithSalmon, copy_tx2gene, copy_false_positives],
       ["expression.dir/fraction_expression.tsv",
        "expression.dir/gene.tpm.tsv",
        "expression.dir/transcript.tpm.tsv"])
def utrons_expression(infiles, outfiles):

    job_memory = "200G"
    script_file = os.path.join(PARAMS["project_src"], 
                               "pipeline_utrons/utrons_Rscript.R")
    statement = '''Rscript %(script_file)s '''
    P.run(statement)
    
    
#------------------------------------------------------------------------------    
@P.cluster_runnable
def low_memory_merge_and_melt(
    fraction_expression,
    gene_tpm,
    transcript_tpm,
    outfile):
    '''merging the gene tpm, the transcript tpm, and the fraction expression
    (each 2-5GB) and then melting the result, takes an extra ordianry amout
    of memory in R, but there is no reason why it has to. This function
    takes each file and outputs a single, tidy format file, with one line
    per transcript per sample, and all three variables, and should use
    next to no memory.'''
    
    frac_expr = iotools.open_file(fraction_expression)
    gene_tpm = iotools.open_file(gene_tpm)
    transcript_tpm = iotools.open_file(transcript_tpm)
    
    outfile = iotools.open_file(outfile, "w")
    
    outfile.write("\t".join(["gene_id", 
                            "transcript_id",
                            "sample_id",
                            "transcript_tpm",
                            "gene_tpm",
                            "fract_expr"]) + "\n")
    
    frac_expr_line = frac_expr.readline()
    gene_tpm_line = gene_tpm.readline()
    transcript_tpm_line = transcript_tpm.readline()
    
    frac_header = frac_expr_line.strip().split()
    gene_header = gene_tpm_line.strip().split()
    transcript_header = transcript_tpm_line.strip().split()
    
    assert frac_header == gene_header == transcript_header
    
    sample_names = frac_header[2:]  
    
    while True:
        frac_expr_line = frac_expr.readline()
        gene_tpm_line = gene_tpm.readline()
        transcript_tpm_line = transcript_tpm.readline() 
        
        if frac_expr_line == "":
            break 
        
        frac_data = frac_expr_line.strip().split()
        gene_data = gene_tpm_line.strip().split()
        transcript_data = transcript_tpm_line.strip().split()
        
        assert frac_data[0] == gene_data[0] == transcript_data[0]
        assert frac_data[1] == gene_data[1] == transcript_data[1]
        
        gene_id = frac_data[0]
        transcript_id = frac_data[1]
        
        for i in range(2, len(frac_data)):
            outfile.write("\t".join([gene_id,
                                     transcript_id,
                                     frac_header[i],
                                     transcript_data[i],
                                     gene_data[i],
                                     frac_data[i]]) + "\n")
    outfile.close()
    

#------------------------------------------------------------------------------    
@merge(utrons_expression,
       "expression.dir/utrons_expression.tsv.gz")
def merge_and_melt_expression(infiles, outfile):
    
    fract_file, gene_file, transcript_file = infiles
    low_memory_merge_and_melt(fract_file, gene_file, transcript_file,
                              outfile,
                              submit=True,
                              job_options = PARAMS["cluster_options"])

    
#------------------------------------------------------------------------------ 
@transform(merge_and_melt_expression,
           suffix(".tsv.gz"),
           ".load")
def load_expression(infile, outfile):
    
    toParquet(infile,
              outfile=outfile,
              unzip=True,
              job_memory="8G")
    
    
#------------------------------------------------------------------------------
@transform(quantifyWithSalmon, 
           regex("quantification.dir/(.+?)\.(.+)/quant.sf"),
           inputs(r"sorted_bams/\1_possorted.bam"),
           r"bigwig.dir/\1.bw")
def make_bigwigs(infile, outfile):
    '''Generate a bigwig depth file'''
    import cgat.BamTools.bamtools as BamTools
    reads_mapped = BamTools.getNumberOfAlignments(infile)
    scale = 1000000.0 / float(reads_mapped)
    tmpfile = os.path.basename(P.get_temp_filename())
    tmpfile2 = os.path.basename(P.get_temp_filename())
    contig_sizes = os.path.join(PARAMS["annotations_dir"], "assembly.dir/contigs.tsv")
    job_memory = "8G"
    statement = '''bedtools genomecov
        -ibam %(infile)s
        -g %(contig_sizes)s
        -bg
        -split
        -scale %(scale)f
        > $TMPDIR/%(tmpfile)s &&
        cat $TMPDIR/%(tmpfile)s | LC_COLLATE=C sort -k1,1 -k2,2n > $TMPDIR/%(tmpfile2)s &&
        bedGraphToBigWig $TMPDIR/%(tmpfile2)s %(contig_sizes)s %(outfile)s &&
        rm -f $TMPDIR/%(tmpfile)s $TMPDIR/%(tmpfile2)s
        '''
        
    P.run(statement)
    

@transform(quantifyWithSalmon, 
           regex("quantification.dir/(.+?)\.(.+)/quant.sf"),
           inputs(r"sorted_bams/\1_possorted.bam"),
           r"sorted_bams/\1_stripped.bam")
def strip_bam(infile, outfile):
    
    statement = '''cgat bam2bam --method=strip-sequence
                        -I %(infile)s -S %(outfile)s -L %(outfile)s.log &&
                        
                    samtools index %(outfile)s'''
                    
    P.run(statement)
    
    
@jobs_limit(1)                   
@collate([merge_and_melt_expression,
        countExonsAndJunctions,
        calculatePSO,
        quantifyWithSalmon,
        make_bigwigs],
        regex("([^/]+)/.+"),
       os.path.join(PARAMS["exportdir"],
                    r"\1/export_sentinal"))
def export_files(infiles, outfile):
    
    indirs = set(inf.split(os.path.sep)[0]
                 for inf in infiles)
    indirs = " ".join(indirs)
    outdir = os.path.dirname(outfile)
    
    statement = '''cp -r %(indirs)s %(outdir)s &&
                   touch %(outfile)s'''
        
    P.run(statement)


@active_if(PARAMS["rmats_run"])
@merge([run_rmats_post,
        run_get_rmats_counts_per_junction],
       os.path.join(PARAMS["exportdir"],
                    "rmats.dir/export_sentinal"))
def export_rmats(infiles, outfile):
    
    copydirs = set(os.path.dirname(inf) for inf in infiles)
    copydirs.remove("rmats.dir")
    copydirs = " ".join(copydirs)
    copyfiles = " ".join(infiles)
    
    outdir = os.path.dirname(outfile)
    
    statement =  '''cp -r %(copydirs)s %(copyfiles)s %(outdir)s &&
                    touch %(outfile)s'''
    P.run(statement)
    

@follows(export_files,
         export_rmats)   
def export():
    pass


# -----------------------------------------------------------------------------
# Generic pipeline tasks
@follows(requant, load_expression, make_bigwigs)
def full():
    pass

####################################################

@follows(mkdir("MultiQC_report.dir"))
@originate("MultiQC_report.dir/multiqc_report.html")
def renderMultiqc(infile):
    '''build mulitqc report'''

    statement = (
        "export LANG=en_GB.UTF-8 && "
        "export LC_ALL=en_GB.UTF-8 && "
        "multiqc . -f && "
        "mv multiqc_report.html MultiQC_report.dir/")

    P.run(statement)


def main(argv=None):
    if argv is None:
        argv = sys.argv
    P.main(argv)


if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
