##############################################################################
#
#   MRC FGU CGAT
#
#   $Id$
#
#   Copyright (C) 2009 Andreas Heger
#
#   This program is free software; you can redistribute it and/or
#   modify it under the terms of the GNU General Public License
#   as published by the Free Software Foundation; either version 2
#   of the License, or (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program; if not, write to the Free Software
#   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
###############################################################################
"""
===========================
Pipeline template
===========================

:Author: Ian Sudbery
:Release: $Id$
:Date: |today|
:Tags: Python
:Updated for usage with python3 by: Cristina Alexandru
:Updated 24/11/2020 by Jack Riley to remove references to deprecated cgatpipelines.report module. Commented out lines: 170, 776-806

Overview
========

The Utrons pipeline is a tool for identifying introns in 3' UTR regions.

The pipeline performs the following:

    * Merges all geneset gtfs with StringTie.
    * Classifies and load transcripts into a databases.
    * Identify transcripts with introns in the 3' UTR, and the location of the introns
    * Annotate the length, splice sites, and distance to stop codon if each intron

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.yml` file.

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_utrons_annotate.py config

Input files
-----------

1. Genesets as GTFs
2. A reference geneset
3. A filtered reference geneset containing only those protein coding 
   transcripts we are confident about.
4. Genome sequence

Requirements
------------

The pipeline requires the results from
:doc:`pipeline_annotations`. Set the configuration variable
:py:data:`annotations_database` and :py:data:`annotations_dir`.

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

.. Add any additional external requirements such as 3rd party software
   or R modules below:

Requirements:

* samtools >= 1.1

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

+--------------+----------+------------------------------------+
|*Program*     |*Version* |*Purpose*                           |
+--------------+----------+------------------------------------+
|gff2fasta     |          |bulding transcriptome in .fa format |
+--------------+----------+------------------------------------+
|salmon_       |>=0.7.2   |building an index                   |
+--------------+----------+------------------------------------+
|salmon_       |>=0.7.2   |alignment-free quantification       |
+--------------+----------+------------------------------------+

Pipeline output
===============

1. Assembled transcripts are generated in .gtf format.
2. A salmon index is built.
3. Salmon quantification files in .sf format are generted in quantification.dir.

Glossary
========

.. glossary::

salmon
      salmon_ - alignment-free quantification

.. _salmon: https://combine-lab.github.io/salmon/

###########################################################################


Code
====

"""
from ruffus import *
from ruffus.combinatorics import product
import sys
import shutil
import sqlite3
import subprocess
import glob
from cgatcore import experiment as E
import cgat.Sra as Sra
import cgatcore.iotools as iotools
from cgatcore import pipeline as P
import tempfile
import os

# load options from the config file
PARAMS = P.get_parameters(
    ["%s/pipeline.yml" % os.path.splitext(__file__)[0],
     "../pipeline.yml",
     "pipeline.yml"])

PARAMS["project_src"]=os.path.dirname(__file__)

# ---------------------------------------------------
@follows(mkdir("final_genesets.dir"))
@merge(["input_genesets.dir/*.gtf.gz",
       PARAMS["annotations_reference_gtf"]],
       "final_genesets.dir/agg-agg-agg.gtf.gz")
def mergeAllAssemblies(infiles, outfile):

    infiles = ["<(zcat %s)" % infile for infile in infiles]
    infiles, reference = infiles[:-1], infiles[-1]

    job_memory = PARAMS["stringtie_merge_memory"]
    job_threads = PARAMS["stringtie_merge_threads"]
    
    infiles = " ".join(infiles)

    statement = '''stringtie --merge
                             -G %(reference)s
                             -p %(stringtie_merge_threads)s
                             %(stringtie_merge_options)s
                             %(infiles)s
                            2> %(outfile)s.log
                   | cgat gtf2gtf --method=sort
                           --sort-order=gene+transcript
                            -S %(outfile)s -L %(outfile)s.log'''

    P.run(statement) 


# ---------------------------------------------------
@transform(["input_genesets.dir/*.gtf.gz", mergeAllAssemblies],
           suffix(".gtf.gz"),
           add_inputs(PARAMS["annotations_reference_gtf"]),
           ".class.gz")
def classifyTranscripts(infiles, outfile):
    '''classify transcripts.
    '''
    to_cluster = True

    infile, reference = infiles

    counter = PARAMS['gtf2table_classifier']

    job_memory = "16G"

    statement = '''
    zcat %(infile)s
    | cgat gtf2table
           --counter=%(counter)s
           --reporter=transcripts
           --gff-file=%(reference)s
           --log=%(outfile)s.log
    | gzip
    > %(outfile)s
    '''
    P.run(statement)

@follows(mkdir("filtered_genesets.dir"))
@transform(classifyTranscripts,
           regex(".+/(.+).class.gz"),
           r"filtered_genesets.dir/\1.filtered.class.gz")
def filterAggClasses(infile, outfile):
    '''Filter merged transcripts to keep only certain classes'''
    
    classes_to_remove = ["intergenic",
                         "complete",
                         "flank3",
                         "flank5",
                         "fragment",
                         "intronic",
                         "utr3", 
                         "utr5"]
    
    outf = iotools.open_file(outfile, "w")
    
    for line in iotools.open_file(infile):
        fields = line.split("\t")
        if fields[0].startswith("MSTR") and fields[6] in classes_to_remove:
            continue
        outf.write(line)
        
    outf.close()
    
    
# ---------------------------------------------------
@follows(mkdir("database_load"), classifyTranscripts)
@merge([classifyTranscripts, filterAggClasses],
       "database_load/transcript_class.load")
def loadTranscriptClassification(infiles, outfile):

    P.concatenate_and_load(infiles, outfile,
                         regex_filename=".+/(.+).class.gz",
                         options="-i transcript_id -i gene_id"
                         " -i match_gene_id -i match_transcript_id"
                         " -i source",
                         job_memory="64G")

# ---------------------------------------------------
@follows(filterAggClasses, mkdir("filtered_genesets.dir"))
@transform(["input_genesets.dir/*.gtf.gz",
            mergeAllAssemblies],
           regex("(.+)/(.+).gtf.gz"),
           add_inputs(r"filtered_genesets.dir/\2.filtered.class.gz"),
           r"filtered_genesets.dir/\2.filtered.gtf.gz")
def filterGTFs(infiles, outfile):
    '''Uses the filtered class file to filter the GTF'''
    
    gtf_file, class_file = infiles
    statement = '''cgat gtf2gtf --method=filter
                                --filter-method=transcript
                                --map-tsv-file=<(zcat %(class_file)s | cut -f1)
                                -I %(gtf_file)s
                                -S %(outfile)s
                                -L %(outfile)s.log'''
    P.run(statement)
 
    
# ---------------------------------------------------
@follows(mkdir("utron_beds.dir"), classifyTranscripts)
@subdivide(filterGTFs,
           regex("(.+)/(.+).filtered.gtf.gz"),
           add_inputs(PARAMS["annotations_filtered_reference_gtf"],
                      r"\1/\2.filtered.class.gz"),
           [r"utron_beds.dir/\2.all_utrons.bed.gz",
            r"utron_beds.dir/\2.indevidual_utrons.bed.gz",
            r"utron_beds.dir/\2.partnered_utrons.bed.gz",
            r"utron_beds.dir/\2.novel_utrons.bed.gz",
            r"utron_beds.dir/\2.no_cds_utrons.bed.gz"])
def find_utrons(infiles, outfiles):

    infile, reference, classfile = infiles
    job_memory="48G"

    all_out, all_bed6_out, part_out, novel_out, no_cds_out = outfiles

    track = P.snip(all_out, ".all_utrons.bed.gz")
    current_file = __file__ 
    pipeline_path = os.path.abspath(current_file)
    pipeline_directory = os.path.dirname(pipeline_path)
    script_path = "pipeline_utrons/find_utrons.py"
    full_utron_path = os.path.join(pipeline_directory, script_path)  
    statement = '''cgat gtf2gtf -I %(infile)s
                             --method=sort
                             --sort-order=gene+transcript
                              -L %(track)s.log
                 | python %(full_utron_path)s 
                             --reffile=%(reference)s
                             --class-file=%(classfile)s
                             --outfile %(all_out)s
                             --indivfile %(all_bed6_out)s
                             --partfile=%(part_out)s
                             --novel-file=%(novel_out)s
                             --not-cds-outfile=%(no_cds_out)s
                              -L %(track)s.log'''

    P.run(statement)


# ---------------------------------------------------
@transform(find_utrons,
           suffix(".bed.gz"),
           ".ids.gz")
def getUtronIds(infile, outfile):

    statement = '''zcat %(infile)s 
                 | cut -f 4
                 | sed 's/:/\\t/g'
                 | sort -u
                 | gzip > %(outfile)s'''

    P.run(statement)


# ---------------------------------------------------
@jobs_limit(1)
@collate(getUtronIds,
         regex(".+/(.+)\.(.+).ids.gz"),
         r"database_load/\2_ids.load")
def loadUtronIDs(infiles, outfile):

    header = "track,transcript_id"
    options = "-i track -i transcript_id"

    if not outfile == "all_utrons_ids.load":
        header += ",match_transcript_id"
        options += " -i match_transcript_id"

    P.concatenate_and_load(infiles, outfile,
                         regex_filename=".+/(.+)\..+\.ids.gz",
                         has_titles=False,
                         cat="track",
                         header=header,
                         options=options,
                         job_memory="60G")

# ---------------------------------------------------
@mkdir("expression.dir")
@transform(filterAggClasses,
           suffix(".class.gz"),
           ".tx2gene.tsv")
def get_tx2gene(infile, outfile):
    '''get a tx2gene file for use in tximport'''
    
    statement = '''zcat %(infile)s
                |  cut -f1,5 
                > %(outfile)s'''
    P.run(statement)
    
    
@follows(loadUtronIDs, loadTranscriptClassification, get_tx2gene)
def AnnotateAssemblies():
    pass


# ---------------------------------------------------
@follows(mkdir("export/indexed_gtfs.dir"))
@transform(filterGTFs,
           regex("(.+)/(.+).gtf.gz"),
           r"export/indexed_gtfs.dir/\2.gtf.gz")
def exportIndexedGTFs(infile, outfile):

    statement = '''zcat %(infile)s
                 | sort -k1,1 -k4,4n
                 | bgzip > %(outfile)s &&
                 tabix -f -p gff %(outfile)s '''

    P.run(statement)


@follows(exportIndexedGTFs)
def export():
    pass


###### Identify splice sites

# ---------------------------------------------------
@follows(mkdir("annotation.dir"))
@transform(find_utrons, 
           regex(".+/(.+-.+-.+)\.(.+)_utrons.bed.gz"), 
           r"annotation.dir/\1_\2_splice_sites.txt")
def identify_splice_sites(infile, outfile):
    
    current_file = __file__
    pipeline_path = os.path.abspath(current_file)
    pipeline_directory = os.path.dirname(pipeline_path)
    
    fastaref = os.path.join(PARAMS["genome_dir"], 
                            PARAMS["genome"] + ".fa")
    
    script_path = os.path.join(pipeline_directory, 
                               "pipeline_utrons/splicesites_start_end_sizes.py")
    
    statement = ''' python %(script_path)s -g %(fastaref)s 
                           -I %(infile)s -O %(outfile)s
                        -L %(outfile)s.log > %(outfile)s.summary'''
    P.run(statement)   


# ---------------------------------------------------
@transform(identify_splice_sites, suffix(".tsv"), ".load")
def load_splice_sites(infile, outfile):
    P.load(infile, 
           outfile, 
           options = "-i transcript_id "
                     "-i ss5 "
                     "-i ss3 "
                     "-i splice_site_start "
                     "-i splice_site_end "
                     "-i utron_size", 
            job_memory="16G")


# ---------------------------------------------------
@follows(mkdir("annotation.dir"))
@transform(PARAMS["annotations_reference_gtf"],
           regex("(.+).gtf.gz"),
           "annotation.dir/gtf_stop_codons.txt")
def gtf_stop_codons(infile, gtf):
    '''Extract stop codon positions from reference gtf'''

    from cgat import GTF
    outfile = open(gtf, "w")
    for line in GTF.iterator(iotools.open_file(infile)):
        if line.feature == "stop_codon":
            if line.strand == "+":
                outfile.write("\t".join(map(str,(line.gene_id,
                                        line.transcript_id,
                                        line.gene_name,
                                        line.strand,
                                        line.feature,
                                        line.contig,
                                        line.start,
                                        line.end))) + "\n")
            elif line.strand == "-":
                outfile.write("\t".join(map(str,(line.gene_id,
                                        line.transcript_id,
                                        line.gene_name,
                                        line.strand,
                                        line.feature,
                                        line.contig,
                                        line.end,
                                        line.start))) + "\n")  

        else:
            pass
    outfile.close()

######### Rscript for generating tpm expression values for transcript and 
# gene level, as well as fraction expression ###########

@follows(mkdir("annotation.dir"))
@transform(PARAMS["database_false_positives"],
           regex(".+"),
           r"annotation.dir/false_positives.txt")
def copy_false_positives(infile, outfile):
    '''copy the file that contains transcripts that get a false positive 3UI
    in simulations to a location when the R script can find it.'''
    
    shutil.copy(infile, outfile)

# --------------------------------------------------------------------------------
@transform(filterGTFs,
           suffix(".gtf.gz"),
           add_inputs("filtered_genesets.dir/agg-agg-agg.filtered.gtf.gz"),
           ".trmap.gz")
def compareGeneSets(infiles, outfile):
    '''Use trmap to compare each geneset to the agg-agg-agg geneset.
    This will allow us to use the agg-agg-agg as a tranferable id
    between sets'''
    
    query_gtf, ref_gtf = infiles
    
    statement = '''trmap <(zcat %(ref_gtf)s) <(zcat %(query_gtf)s)
                     | gzip > %(outfile)s'''
    
    P.run(statement)                 
                     
# ---------------------------------------------------------------------------------
@P.cluster_runnable
def trmap2tsv(infile, outfile):
    
    outf = iotools.open_file(outfile, "w")
    outf.write("ref_id\tquery_id\n")
    
    current_id = None
    
    for line in iotools.open_file(infile):
        fields = line.strip().split()
        
        if line.startswith(">"):
            query_id = fields[0][1:]
            continue
        
        if fields[0] == "=":
            outf.write("\t".join([fields[5], query_id]) + "\n")
            
    outf.close()
    
    
@transform(compareGeneSets,
           suffix(".trmap.gz"),
           ".matches.tsv.gz")
def run_trmap2tsv(infile, outfile):
    '''Convert the trmap pseduo fasta into a two column tsv that lists the 
    geneset, the transcript id in agg-agg-agg and the transcript_id in the track
    in question, filtering to only "=" transcripts. '''
    
    track = P.snip(os.path.basename(infile), ".trmap.gz")
    
    trmap2tsv(infile, outfile, submit=True)
    

@merge(run_trmap2tsv, "transcript_map.load")
def load_geneset_comparisons(infiles, outfile):
    '''Take the tables created from the trmaps and load them into a single
    table, keyed on the geneset they came from'''
    
    P.concatenate_and_load(infiles, outfile,
                           regex_filename=".+/(.+).filtered.matches.tsv.gz",
                           options = "-i track -i ref_id")
    

# --------------------------------------------------------------------------------------------------------------------------------------------
# Generic pipeline tasks
@follows(export, identify_splice_sites, gtf_stop_codons, AnnotateAssemblies)
def full():
    pass

####################################################

def main(argv=None):
    if argv is None:
        argv = sys.argv
    P.main(argv)


if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
